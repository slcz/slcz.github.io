---
layout: post
title:  "Write Amplification, Randomness and Math"
date:   2016-02-02 13:15:31 -0800
categories:
tags: ["SSD", "Flash", "Write Amplification"]
---

Imagine sitting inside a sushi restaurant, where shshi plates are placed on a slowly moving conveyor bar. Customer picks up full plates and leaves empty ones on the bar. Suppose a waiter is standing at the end of conveyor and moving empty plates to the kitchen, meanwhile keeps adding new plates into the loop. How fast should the waiter moves the plates in order to keep the bar full without leaving empty plates on the bar for too long?

<img alt="" width="400" src="/assets/sushi.jpg"/>

From time to time, while working on flash controller and SSDs, I encountered the same question. In a somewhat similar situation as the sushi bar metaphor, a flash device is written sequentially regardless of incoming write's address pattern. Considering a simplified case, suppose external users (applications) write to the device consistently and the workload is randomly distributed across the entire usable space. How fast should the SSD move fragmented blocks around in order to keep up with the data coming rate?

We know SSD exposes smaller capacity to the applications than the physical capacity. Overtime, a more recent write with the same user address may overwrites one or more older writes with the same user address, hence creating "holes" and fragmentation. A background garbage collection actively defragment the "holes". For a given workload, the more the over-provisioning space reservation, the less work SSD's garbage collector has to do to re-pack the working set. The trade-off of having more space reservation is the wasteful of flash capacity.

Modern flash controller inside SSD may employ sophisticated garbage collection algorithm. Some of them exploit spatial locality, e.g., they may try to pack infrequently accessed data together in order to reduce the write amplification (back-end writes to front-end writes ratio). For instance, a multiple streams GC algorithm writes cold data into a separate stream.

<img alt="multiple stream GC" width="500" src="/assets/gc.png"/>

While it is understandable that an intelligent algorithm may take advantage of workload patterns, sometimes we are interested in the case where the entire write workload (both in terms of address and data pattern) is randomly distributed. This provides a fair ground when benchmarking the I/O throughput. It is used in micro-benchmarking such as [_fio_][fio].

Another point that may not be so obvious is that in terms of distribution of holes, a randomly generated address pattern is different from uniformly strided (stride > 1) address pattern. For write addresses that are generated by a true random generator, an old aged block has better probability of being overwritten by younger user writes. Younger blocks are less possible. A uniformly strided address pattern causes all the blocks on the device to have the same probability of being "staled".

<img alt="fragmentation distribution" width="500" src="/assets/holes.png"/>

In a perfectly randomly distributed workload where there is no obvious distinction of address pattern locality, multiple-queue GC algorithm won't work any better than a naive single queue, round-robin GC algorithm. Therefore, I choose the simpler GC to model random workload write amplification. Also, I choose to model the GC algorithm as being lazy, i.e., GC only happens when there is no free space left. In another words, gc-threshold is close to zero.

<img alt="multiple stream GC" width="500" src="/assets/naive_gc.png"/>

While it is possible to run a simulator and get write amplification distribution on space over-provisioning, wouldn't it be intersting to try to derive the model analytically? We know when a block is finally picked up for garbage collection, we could calculate the probability that it is not overwritten by a newer write. As below,

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            P_{valid}=(1-\frac{1}{U})^W
            </script>
        </td>
        <td class="equation-number">
            (1)
        </td>
    </tr>
</table>
Where,

- $$P_{valid}$$ is the probability that the block is still valid when it's
scanned for GC.
- $$U$$ is the total amount of usable blocks
- $$W$$ is the total number of of younger writes coming from external user

Due to lazy GC policy, $$U, W$$ are both very large numbers that are in the same magnitude as the total physical capacity in the device (millions to billions of blocks). We further introduce 2 variables, $$f$$ and $$\lambda$$. $$f$$
 is the ratio of usable capacity to physical capacity. $$\lambda$$,
defined as the ratio of external writes to total device writes, is the
inverse of write amplification, $$\omega$$.

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            U=fS
            </script>
        </td>
        <td class="equation-number">
            (2)
        </td>
    </tr>
</table>

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            W=\lambda S=\frac{S}{\omega}
            </script>
        </td>
        <td class="equation-number">
            (3)
        </td>
    </tr>
</table>

From _Eq._ (1), (2) and (3), we get,

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            P_{valid}=(1-\frac{1}{U})^W=
            (1-\frac{\lambda}{f}\frac{1}{\lambda S})^{W}=
            (1-\frac{\lambda}{f}\frac{1}{W})^W
            </script>
        </td>
        <td class="equation-number">
            (4)
        </td>
    </tr>
</table>

In the case where garbage collection is lazy, $$W$$ is large. As $$W$$
approaches infinity, the page valid probability becomes,

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            \Psi=\lim_{W \to \infty} P_{valid}=\lim_{W \to \infty}(1-\frac{\lambda}{f}\frac{1}{W})^W=e^{-\frac{\lambda}{f}}
            </script>
        </td>
        <td class="equation-number">
            (5)
        </td>
    </tr>
</table>

$$\Psi$$ approximates the valid probability of the earliest block written in the system. _Eq._ (5) is due to the fact that
<script type="math/tex"> \lim_{n \to \infty} (1+\frac{k}{n})^n = e^k</script>.
Further, write amplification has the following relationship to the block valid probability,

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            \omega=\frac{1}{\lambda}=\frac{1}{1-\Psi}
            </script>
        </td>
        <td class="equation-number">
            (6)
        </td>
    </tr>
</table>

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            \Psi=1-\lambda
            </script>
        </td>
        <td class="equation-number">
            (7)
        </td>
    </tr>
</table>

Put _Eq._ (5) and (7) together,

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            1-\lambda=e^{-\frac{\lambda}{f}}
            </script>
        </td>
        <td class="equation-number">
            (8)
        </td>
    </tr>
</table>

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            1-\frac{1}{\omega}=e^{-\frac{1}{f\omega}}
            </script>
        </td>
        <td class="equation-number">
            (9)
        </td>
    </tr>
</table>

Where,

- $$\omega$$ is the write amplification,
- $$f$$ is the ratio of physical capacity to usable capacity.

Notice _Eq._ (9) does not have elementary solution, but it can be solved by
using [lambertW function][lamberW].

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            \omega=\frac{\zeta}{\zeta-W(\zeta e^\zeta)}
            </script>
        </td>
        <td class="equation-number">
            (10)
        </td>
    </tr>
</table>

<table class="numbered-equation" cellpadding="0" cellspacing="0">
    <tr>
        <td>
            <script type="math/tex; mode=display">
            \zeta=-\frac{1}{f}
            </script>
        </td>
        <td class="equation-number">
            (11)
        </td>
    </tr>
</table>

_Eq._ (10) and (11) solves write amplification as a function of usable 
ratio. With a good [LambertW function approximator][approx], we can
calculate,

| usable-ratio | write-amplification |
|--------------:| ------------------: |
| 95%           | 10.17               |
| 90%           | 5.18                |
| 85%           | 3.52                |
| 80%           | 2.69                |
| 75%           | 2.20                |
| 70%           | 1.88                |
| 65%           | 1.65                |
| 60%           | 1.48                |
| 55%           | 1.35                |
| 50%           | 1.26                |

<img alt="write amp to usable ratio" width="800" src="/assets/wa.png"/>

This matches very well with real life observation. In reality, most of the
consumer grade SSDs are thinly over-provisioned. Most of them leave less than
10% of capacity reservation, causing 5-10X performance degradation under 
write stress workload. You could tell how much usable space does a particular SSD provision by observing their steady state performance
to peak performance ratio. However, don't forget to pre-condition the SSD
by filling the SSD with data before any stress testing. Otherwise, it may
take very long for the performance to stabilize.

Other than refreshing the math skills, the above analytical exercise may
also help space planing when using SSDs. For example, in an anticipated
write heavy environment, one could provide more predictable throughput
by intentionally format the file system at lower (than advertised) capacity.
Next time, format the drive at 70% of the advertised capacity if you want
to ensure the worst case throughput is better than 50% of the peak!

[lamberW]: https://en.wikipedia.org/wiki/Lambert_W_function
[approx]:  http://keithbriggs.info/software/LambertW.c
[w2f]:     /assets/wa.png
[fio]:     http://freecode.com/projects/fio
